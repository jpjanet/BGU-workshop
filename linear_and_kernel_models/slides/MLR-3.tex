 It is worth briefly considering when the inverse in (\ref{eq:normaleq}) exists. In general $X^TX$ is a positive semidefinite matrix, and only has zero eigenvalues (remember, if a matrix has zero eigenvalues, it is not invertible) if there are vectors in the (right)-nullspace of $X$, i.e. if there are vectors $v$ such that $Xv=0$. This is only the case if the row-rank of $X$ is less than than the number of rows $n$ - this can occur if two exact rows or columns are copies of each other (or linear combinations). This is somewhat unlikely in applications where each observation corresponds to a different molecule or configuration. In cases where the number of features $d$ are larger than the number of observations $n$, the inverse in non-unique and this degeneracy must be broken usually by setting some weights to zero. What is a more likely occurrence in practice is that the rows of $X$ will be \textit{nearly co-linear}, in which case the the eigenvalues of the matrix $X^TX$ will be close to zero and the resulting weights will be unstable and change drastically in response to small perturbations in the training data\footnote{as can be understood though the condition number of matrix, see any text on numerical linear algebra for more details}.  This occurs frequently when studying molecular systems since many molecules are fundamentally similar -- for example, a data set might include the two molecules differing only by a single hydrogen atom, and in many representations (see Chapter \ref{chap:reprsentations}) the fingerprints associated with these two molecules will be very similar. 

The answer to scaling problems of the matrix inverse comes from the regularization term $\lambda$. Note the eigenvalues of  $X^TX + I_{d}\lambda$ are just\footnote{$X^TX +  I_{d}\lambda = D^{-1}\Omega D +   D^{-1}D\lambda = D^{-1}\left(\Omega + I_{d}\lambda\right)D$, a  property for all sums of commutative matrices} $\omega_i + \lambda$,  where $\omega_i$ are the eigenvalues of $X^TX$, i.e. the regularization term shifts all of the eigenvalues by a fixed amount and therefore always improves the conditioning of the least squares matrix. Even a small positive value of $\lambda$ will typically be enough to `fix' any nearly co-linear matrix elements since it will shift the smallest eigenvalues away from zero. In addition to this, $\lambda$ also serves the regular function of reducing the coefficients; in the context of linear models Eq. \ref{Eq:reg_l2l2} implies that slopes of the linear model will be chosen to be closer to zero (i.e. the model changes more slowly as a function of the input) when using $\lambda >0$.