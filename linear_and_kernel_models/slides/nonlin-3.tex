Notice that all that is required is vector products, i.e.
\begin{align*}
K_{i,j} =  \left\langle \varphi(x_i),\varphi(x_j) \right\rangle 
\end{align*}  \\$ $\\ It turns out that these can often be computed \emph{much} more efficiently, without ever forming the big, nonlinear feature space directly. \pause{}For our quadratic example:
\begin{align*}
K_{i,j} = \left((x^{(i)})^Tx^{(j)} +1\right)^2 = (x_{1}^{(i)})^2(x_{1}^{(j)})^2  + 2x_{1}^{(i)}x_{1}^{(j)}x_{2}^{(i)}x_{2}^{(j)}+ \hdots 
\end{align*}
\pause{}
which {can be computed entirely using vectors in} $\mathbb{R}^2$, so we never have to allocate the (factorially large) feature space.\\

